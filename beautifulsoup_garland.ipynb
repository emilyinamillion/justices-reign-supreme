{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main scraping code for GARLAND in cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I needed to do Garland's opinions separately because his sources were more disperate than the justices. I ultimately decided to only use his constitutional law opinions because those are obviously the most relevant to our needs for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def graboplinks():\n",
    "    caselist = [\"civil_rights\",\"constitutional-law\", \"criminal-law\", \"administrative-law\"]\n",
    "    root_url = 'http://www.merrickgarlandproject.com/'\n",
    "    all_links = {}\n",
    "    \n",
    "    for case_type in caselist:\n",
    "        response = root_url + case_type\n",
    "        \n",
    "    #getting info from urls\n",
    "        resp = requests.get(response)\n",
    "        resp = BeautifulSoup(resp.text, \"lxml\")\n",
    "        article = resp.find(\"article\")\n",
    "        \n",
    "        \n",
    "        #grabs only the links on the page with read opinion\n",
    "        #blocking pdfs bc pdfs are lame\n",
    "        for link in article.findAll(\"a\", text=re.compile(\"Read Opinion\")):\n",
    "            url = link[\"href\"]\n",
    "            if not url.endswith(\"pdf\"):\n",
    "                response = requests.get(url)\n",
    "                page = response.text\n",
    "                soup = BeautifulSoup(page, \"lxml\")\n",
    "                \n",
    "                #goes to justia site if it's a justia link\n",
    "                if re.search(\"justia\", url):\n",
    "                    pagesoup = soup.find_all(class_=\"tab-content\")    \n",
    "                    for element in pagesoup:\n",
    "                        text = element.get_text()\n",
    "                        dummytext = text + \" \"\n",
    "                        if re.search(\"dissenting opinion filed by circuit judge garland\", str(pagesoup).lower()):\n",
    "                            opinion, dissent = text.split(\"GARLAND, Circuit Judge, dissenting\")\n",
    "                            all_links[url] = [case_type]\n",
    "                            all_links[url].append(\"dissent\")\n",
    "                            all_links[url].append(dissent)\n",
    "                        else:\n",
    "                            all_links[url] = [case_type] \n",
    "                            all_links[url].append(\"opinion\")\n",
    "                            all_links[url].append(dummytext) \n",
    "                \n",
    "                #goes to openjurist site if it's an openjurist link\n",
    "                elif re.search(\"openjurist\", url):          \n",
    "                    pagesoup = soup.find_all(class_=\"node-content clear-block prose\")    \n",
    "                    for element in pagesoup:\n",
    "                        text = element.get_text()\n",
    "                        dummytext = text + \" \"\n",
    "                        if re.search(\"dissenting opinion filed by circuit judge garland\", str(pagesoup).lower()):\n",
    "                            opinion, dissent = text.split(\"GARLAND, Circuit Judge, with whom\")\n",
    "                            all_links[url] = [case_type]\n",
    "                            all_links[url].append(\"dissent\")\n",
    "                            all_links[url].append(dissent)\n",
    "                        else:\n",
    "                            all_links[url] = [case_type]\n",
    "                            all_links[url].append(\"opinion\")\n",
    "                            all_links[url].append(dummytext)\n",
    "                            \n",
    "                elif re.search(\"leagle\", url):          \n",
    "                    pagesoup = soup.find_all(class_=\"decision-text-content\")\n",
    "                    dummytext = text + \" \"\n",
    "                    for element in pagesoup:\n",
    "                        text = element.get_text()\n",
    "                        all_links[url] = [case_type]\n",
    "                        all_links[url].append(\"opinion\")\n",
    "                        all_links[url].append(dummytext)\n",
    "                #if link type not found, just append blank stuff\n",
    "                \n",
    "                elif re.search(\"titleii\", url):          \n",
    "                    article = soup.find(\"p\")\n",
    "                    article = article.get_text()\n",
    "                    opinion, dissent = article.split(\"Garland, Circuit Judge, dissenting\")\n",
    "                    all_links[url] = [case_type]\n",
    "                    all_links[url].append(\"dissent\")\n",
    "                    all_links[url].append(dissent)\n",
    "                else:\n",
    "                    all_links[url] = [case_type]\n",
    "                    all_links[url].append(\"\")\n",
    "                    all_links[url].append(\"\")        \n",
    "                        \n",
    "\n",
    "    return all_links\n",
    "\n",
    "rawdata = graboplinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caseurl</th>\n",
       "      <th>type</th>\n",
       "      <th>ruled</th>\n",
       "      <th>text</th>\n",
       "      <th>iscasetext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://law.justia.com/cases/federal/appellate-...</td>\n",
       "      <td>civil_rights</td>\n",
       "      <td>opinion</td>\n",
       "      <td>U.S. Court of Appeals for the District...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             caseurl          type    ruled  \\\n",
       "0  http://law.justia.com/cases/federal/appellate-...  civil_rights  opinion   \n",
       "\n",
       "                                                text  iscasetext  \n",
       "0          U.S. Court of Appeals for the District...           0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(rawdata)\n",
    "df = df.transpose().reset_index()\n",
    "#dfcasetext = df[df[\"\"]]\n",
    "\n",
    "def casetext(case):\n",
    "    if re.search(\"casetext\", case):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def cleanerup(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    text = text.replace(\"\\\\\", \" \")\n",
    "    return text\n",
    "\n",
    "df.columns = [\"caseurl\", \"type\", \"ruled\", \"text\"] \n",
    "\n",
    "df[\"iscasetext\"] = df.caseurl.apply(casetext)\n",
    "df[\"text\"] = df.text.apply(cleanerup)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a casetitle + year dataframe separately to merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caseurl</th>\n",
       "      <th>casetitle</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://casetext.com/case/malik-v-dist-of-colu...</td>\n",
       "      <td>Malik v. District of Columbia</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             caseurl  \\\n",
       "0  https://casetext.com/case/malik-v-dist-of-colu...   \n",
       "\n",
       "                        casetitle  year  \n",
       "0  Malik v. District of Columbia   2009  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caselist = [\"civil_rights\",\"constitutional-law\", \"criminal-law\", \"administrative-law\"]\n",
    "root_url = 'http://www.merrickgarlandproject.com/'\n",
    "all_links = []\n",
    "titles = []\n",
    "years = []\n",
    "for case_type in caselist:\n",
    "    response = root_url + case_type\n",
    "    resp = requests.get(response)\n",
    "    resp = BeautifulSoup(resp.text, \"lxml\")\n",
    "    article = resp.find(\"article\")\n",
    "    for link in article.findAll(\"a\", text=re.compile(\"Read Opinion\")):\n",
    "            url = link[\"href\"]\n",
    "            all_links.append(url)\n",
    "    for title in resp.find_all(\"h3\"):\n",
    "        title = title.get_text().replace(\"\\n\", \"\")\n",
    "        title, year = title.split(\"(\" )\n",
    "        year = year.replace(\")\", \"\")\n",
    "        titles.append(title)\n",
    "        years.append(year)\n",
    "        \n",
    "casetitles = pd.DataFrame(titles, all_links).reset_index()\n",
    "casetitles[\"years\"] = years\n",
    "casetitles.columns = [\"caseurl\", \"casetitle\", \"year\"]\n",
    "casetitles.head(1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfwithtitles = pd.merge(casetitles, df, on=\"caseurl\", how= \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#splitting dfs into two to track down casetext docs in an easier format\n",
    "df2, casetext = dfwithtitles[dfwithtitles[\"iscasetext\"] == 0], dfwithtitles[dfwithtitles[\"iscasetext\"] == 1]\n",
    "casetext = casetext[casetext[\"type\"] == \"constitutional-law\"]\n",
    "#casetext\n",
    "df2.to_pickle(\"Garland_firstpass.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lost a lot of constitutional law opinions because casetext isn't scrapeable. \n",
    "Manually grabbed a handful of links for constitutional law opinions from one of the cleaner sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grabconlaw():\n",
    "    datum = {}\n",
    "    links = [\"http://www.leagle.com/decision/In%20FCO%2020140715151/MPOY%20v.%20RHEE\",\n",
    "            \"http://www.leagle.com/decision/20081366532F3d834_11291/PARHAT%20v.%20GATES\",\n",
    "         \"http://www.leagle.com/decision/20031385323F3d1062_11284/RANCHO%20VIEJO,%20LLC%20v.%20NORTON\",\n",
    "         \"http://www.leagle.com/decision/In%20FDCO%2020160426869/IN%20RE%20ETHICON,%20INC.,%20PELVIC%20REPAIR%20SYSTEM%20PRODUCTS%20LIABILITY%20LITIGATION\",\n",
    "         \"http://www.leagle.com/decision/In%20FCO%2020101018069/FREEDOM%20HOLDINGS,%20INC.%20v.%20CUOMO\",\n",
    "         \"http://www.leagle.com/decision/19951435903FSupp532_11348/STATE%20OF%20NEW%20YORK%20BY%20VACCO%20v.%20REEBOK%20INTERN.%20LTD.\",\n",
    "         \"http://www.leagle.com/decision/19921205959F2d246_11147/U.S.%20v.%20HARRIS\"]\n",
    "             \n",
    "    for link in links:\n",
    "        response = requests.get(link)\n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page, \"lxml\")    \n",
    "        title = pageheader(soup)\n",
    "        info = pagetext(soup)\n",
    "        info.append(link)\n",
    "        datum[title] = info\n",
    "    return(datum)    \n",
    "    \n",
    "data = grabconlaw()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pageheader(soup):\n",
    "    headers = soup.find_all(\"h1\")\n",
    "    for element in headers:\n",
    "        header = element.get_text()\n",
    "        header = header.replace(\"\\n\", \"\")\n",
    "        header = header.replace(\"\\t\", \"\")\n",
    "        title = header\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pagetext(soup):\n",
    "    info = []\n",
    "    \n",
    "    pagesoup = soup.find_all(class_=\"decision-text-content\")\n",
    "    for element in pagesoup:\n",
    "        text = element.get_text()\n",
    "        dummytext = text + \" \"\n",
    "        \n",
    "        if re.search(\"dissenting opinion filed by circuit judge garland\", str(pagesoup).lower()):\n",
    "            opinion, dissent = text.split(\"GARLAND, Circuit Judge, dissenting\")\n",
    "            info.append(\"constitutional-law\")\n",
    "            info.append(\"dissent\")\n",
    "            info.append(dissent)\n",
    "        else:\n",
    "            info.append(\"constitutional-law\")\n",
    "            info.append(\"opinion\")\n",
    "            info.append(dummytext)\n",
    "            \n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def casetext(case):\n",
    "    if re.search(\"casetext\", case):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def cleanerup(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    text = text.replace(\"\\\\\", \" \")\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the two dataframes the same to concatenate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_pickle(\"Garland_firstpass.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = df2[df2[\"ruled\"] != \"\"]\n",
    "df2 = df2[['caseurl', 'casetitle', 'year', 'type', 'ruled', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"year\"] = 0\n",
    "df = df[['caseurl', 'casetitle', 'year', 'type', 'ruled', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "garland = pd.concat([df2, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "garland.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "garland[\"casetitle\"] = garland.casetitle.apply(cleanerup)\n",
    "garland[\"text\"] = garland.text.apply(cleanerup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "garland.to_pickle(\"fullgarland.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post process review\n",
    "I found a corpus of data that educated lawers thought were relevant for Garland's qualifications to be a supreme court justice. On this site, these lawyers had compiled a list of urls from disperate sites with opinions and dissents of Garland. A huge challenge with this was to figure out how to break text up for topics I don't understand (I'm not a lawyer), but I was able to deduce what was going on in each document and figure out how to break up the texts based on patterns. \n",
    "\n",
    "__Next time__\n",
    "-  I'd find one good website for all of this person's text! However, this was a really good learning process for BeautifulSoup because I'd had a hard time with that in the past.\n",
    "-  I'd be sure to get the descriptive info at the same time as the body of text to prevent later headaches. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
