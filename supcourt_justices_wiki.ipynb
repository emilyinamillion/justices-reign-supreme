{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the justice bio info\n",
    "Wikipedia is a great source for summary info of a lot of things. Their lack of javascript is very inviting for webscraping (however, you can't really write a script to iterate through pages on wikipedia since there are thousands of different authors, therefore many ways of presenting the same problem. So the best use for wikipedia is if you need a table or two from a single page). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "import html5lib\n",
    "import re\n",
    "tbls1 = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_Justices_of_the_Supreme_Court_of_the_United_States\")\n",
    "tbls1 = tbls1[1]\n",
    "df = pd.DataFrame(tbls1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns = [\"dummy\", \"name\", \"State\", \"lifetime\", \"service\", \"ChiefJus\", \"Retired\", \"Appointedby\", \"termination\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lastname(text):\n",
    "    lastname = text.split(\",\")\n",
    "    return(lastname[0])\n",
    "\n",
    "def service(text):\n",
    "    service = str(text).split(\"(\")\n",
    "    return(service[0])\n",
    "\n",
    "\n",
    "\n",
    "def replacestuff(text):\n",
    "    pattern = text.find(\"........-\")\n",
    "    text = str(text)\n",
    "    text.replace(pattern.group(), \"\")\n",
    "\n",
    "    \n",
    "df[\"lastname\"] = df.name.apply(lastname)\n",
    "df[\"serviceyrs\"] = df.service.apply(service)\n",
    "\n",
    "df[\"lastname\"] = df[\"lastname\"].map(lambda x: x.upper())\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.lastname.value_counts().to_csv(\"cleanedsupjustices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def start(text):\n",
    "    text = list(str(text))\n",
    "    serve1 = text[0:4]\n",
    "    return ''.join(serve1)\n",
    "\n",
    "def end(text):\n",
    "    text = str(text)\n",
    "    text = text.split(' ')\n",
    "    text = text[0]\n",
    "    serve1 = text[-4:]\n",
    "    serve1 = serve1.replace('sent', 'now')\n",
    "    return str(serve1)\n",
    "df['start'] = df.service.apply(start)\n",
    "df['end'] = df.service.apply(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_pickle('justicesummary.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We also need the political affiliations of the people who nominated these justices\n",
    "I didn't include this info for analysis (at this time), I included it because I thought it would be dope to see. This page was hilariously broken up so I wrote a script for each one. If there were any more than there were I would have written a function, but I was already half way through by the time I thought about doing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "import html5lib\n",
    "import re\n",
    "tbls1 = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_Presidents_of_the_United_States_by_political_affiliation\")\n",
    "republicans = tbls1[1]\n",
    "dems = tbls1[2]\n",
    "waffles = tbls1[3]\n",
    "whigs = tbls1[4]\n",
    "federalists = tbls1[5]\n",
    "nonparty = tbls1[6]\n",
    "repu = pd.DataFrame(republicans)\n",
    "demos = pd.DataFrame(dems)\n",
    "waff = pd.DataFrame(waffles)\n",
    "whig= pd.DataFrame(whigs)\n",
    "fed= pd.DataFrame(federalists)\n",
    "nonparty= pd.DataFrame(nonparty)\n",
    "repu['party'] = 'republican'\n",
    "demos['party'] = 'democrat'\n",
    "waff['party'] = 'waffles'\n",
    "whig['party'] = 'whigs'\n",
    "fed['party'] = 'fed'\n",
    "nonparty['party'] = 'nonpartisan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#these tables were weird when they came in, this fixed them\n",
    "repu = repu.reindex(repu.index.drop(0))\n",
    "demos = demos.reindex(demos.index.drop(0))\n",
    "waff = waff.reindex(waff.index.drop(0))\n",
    "whig = whig.reindex(whig.index.drop(0))\n",
    "fed = fed.reindex(fed.index.drop(0))\n",
    "nonparty = nonparty.reindex(nonparty.index.drop(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.concat([repu, demos, waff, whig, fed, nonparty]).to_pickle('politicalparties.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
